{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6e53d7-ab53-4e79-b942-8159618300c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### deserialize Avro using writer schema, validate, watermark, dedup, route errors to DLQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9aebc34-8505-45a9-8dfc-e17a3a44f83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Avro decode helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a615a9f-201a-4a12-9c69-c2bc1d2c29e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ac3641f-bd4b-4e48-a4f4-8fb670679cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use schema fetched earlier as `writer_schema`\n",
    "# If you prefer static, paste schema JSON into a string variable.\n",
    "\n",
    "bronze_stream = spark.readStream.format('delta').load(bronze_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018f9ccd-d351-4ebd-baba-80a92fa36c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try decode; if it fails for the whole batch, handle in foreachBatch\n",
    "decoded = bronze_stream.select(\n",
    "    F.col('topic'), F.col('partition'), F.col('offset'), F.col('kafka_ts'),\n",
    "    F.col('value_bytes'),\n",
    "    from_avro('value_bytes', writer_schema).alias('data')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4064cb95-666c-4e37-9cf4-ca35809da4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flatten expected columns (example order schema)\n",
    "flat = decoded.select(\n",
    "    'topic','partition','offset','kafka_ts',\n",
    "    F.col('data.order_id').alias('order_id'),\n",
    "    F.col('data.event_time').cast('timestamp').alias('event_ts'),\n",
    "    F.col('data.customer_id').alias('customer_id'),\n",
    "    F.col('data.amount').cast('double').alias('amount'),\n",
    "    F.col('data.currency').alias('currency')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbef3ec4-e213-4e19-baba-5114ee8c2c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Watermark & basic DQ\n",
    "validated = (flat\n",
    "  .withWatermark('event_ts', '15 minutes')\n",
    "  .filter('amount IS NOT NULL AND amount >= 0')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bc1402-d878-4334-957c-e81d36453427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate by business key within watermark\n",
    "silver = validated.dropDuplicates(['order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2d1aa8b-e278-4bc5-ba9c-9a74649b08f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = 'dbfs:/pipelines/orders/silver'\n",
    "ckpt_silver = 'dbfs:/pipelines/orders/_ckpt_silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a796d83-fd62-49fa-aab9-f3968a2760d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (silver.writeStream\n",
    "  .format('delta')\n",
    "  .outputMode('append')\n",
    "  .option('checkpointLocation', ckpt_silver)\n",
    "  .start(silver_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0433e013-4721-4c77-8022-43bd0d502b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DLQ handling pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5af43cb-5498-45c3-b9c9-2b09b831ec11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlq_path = 'dbfs:/pipelines/orders/dlq'\n",
    "ckpt_dlq = 'dbfs:/pipelines/orders/_ckpt_dlq'\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def process_batch(batch_df: DataFrame, batch_id: int):\n",
    "    try:\n",
    "        parsed = (batch_df\n",
    "          .select('topic','partition','offset','kafka_ts','value_bytes',\n",
    "                  from_avro('value_bytes', writer_schema).alias('data')))\n",
    "        # If the above fails, itâ€™ll jump to except\n",
    "        flattened = (parsed\n",
    "          .select('topic','partition','offset','kafka_ts',\n",
    "                  F.col('data.*')))\n",
    "        (flattened\n",
    "          .write.mode('append')\n",
    "          .format('delta')\n",
    "          .save(silver_path))\n",
    "    except Exception as e:\n",
    "        # Persist batch to DLQ with error\n",
    "        err_str = str(e)\n",
    "        (batch_df\n",
    "          .withColumn('error', F.lit(err_str))\n",
    "          .write.mode('append')\n",
    "          .format('delta')\n",
    "          .save(dlq_path))\n",
    "\n",
    "(bronze_stream.writeStream\n",
    "  .foreachBatch(process_batch)\n",
    "  .option('checkpointLocation', ckpt_dlq)\n",
    "  .start())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_decode_clean",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
